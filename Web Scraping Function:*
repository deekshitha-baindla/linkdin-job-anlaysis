import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

def scrape_linkedin_search_page(url, headers):
    """Scrapes job titles and locations from a LinkedIn search page."""
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise an exception for bad status codes
        soup = BeautifulSoup(response.content, 'html.parser')
        job_listings = soup.find_all('div', class_='base-card relative w-full hover:bg-gray-lightest p-2 pb-2 border-b border-solid border-gray-200') # Example class - inspect actual LinkedIn HTML

        data = []
        for job in job_listings:
            title_element = job.find('h3', class_='base-search-card__title')
            location_element = job.find('span', class_='job-search-card__location')

            title = title_element.text.strip() if title_element else None
            location = location_element.text.strip() if location_element else None

            # Attempt to extract skills (this might be less reliable from the search page)
            skills_elements = job.find_all('li', class_='job-card-search__item--primary-skill') # Example class
            skills = [skill.text.strip() for skill in skills_elements] if skills_elements else []

            data.append({'Title': title, 'Location': location, 'Skills': skills})

        return pd.DataFrame(data)

    except requests.exceptions.RequestException as e:
        print(f"Error during request: {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"An error occurred: {e}")
        return pd.DataFrame()

def get_linkedin_search_urls(keywords, locations, base_url='https://www.linkedin.com/jobs/search/?'):
    """Generates LinkedIn search URLs for given keywords and locations."""
    urls = []
    for keyword in keywords:
        for location in locations:
            encoded_keyword = keyword.replace(' ', '%20')
            encoded_location = location.replace(' ', '%20')
            url = f"{base_url}keywords={encoded_keyword}&location={encoded_location}"
            urls.append(url)
    return urls

# Example usage:
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
keywords = ['Software Engineer', 'Data Scientist']
locations = ['Hyderabad', 'Bangalore']
search_urls = get_linkedin_search_urls(keywords, locations)

all_job_data = []
for url in search_urls:
    print(f"Scraping: {url}")
    df = scrape_linkedin_search_page(url, headers)
    if not df.empty:
        all_job_data.append(df)
    time.sleep(random.uniform(1, 3)) # Be respectful with delays

if all_job_data:
    combined_df = pd.concat(all_job_data, ignore_index=True)
    print(combined_df.head())
else:
    print("No job data scraped.")
